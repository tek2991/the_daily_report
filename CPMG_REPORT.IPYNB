{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import smtplib\n",
    "from bs4 import BeautifulSoup\n",
    "from email.message import EmailMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Variables\n",
    "cname = \"Assam\"\n",
    "\n",
    "date = \"02/05/2023\"\n",
    "mobilink_date = date\n",
    "\n",
    "if date == \"\":\n",
    "    date = datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "if mobilink_date == \"\":\n",
    "    mobilink_date = datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "sender_email = \"techassam@gmail.com\"\n",
    "sender_password = \"\"\n",
    "recepients = [\n",
    "    # \"dodarrang.as@indiapost.gov.in\",\n",
    "    # \"docachar.as@indiapost.gov.in\",\n",
    "    # \"donalbari.as@indiapost.gov.in\",\n",
    "    # \"dotinsukia.as@indiapost.gov.in\",\n",
    "    # \"doguwahati.as@indiapost.gov.in\",\n",
    "    # \"dogoalpara.as@indiapost.gov.in\",\n",
    "    # \"dodibrugarh.as@indiapost.gov.in\",\n",
    "    # \"dosibsagar.as@indiapost.gov.in\",\n",
    "    # \"donagaon.as@indiapost.gov.in\"\n",
    "    # 'eliezer.kt@gmail.com',\n",
    "    # 'tek2991@gmail.com',\n",
    "]\n",
    "\n",
    "reports = {\n",
    "    '1' : 'Details of Post Offices having daily synch failure for more than 48 hours',\n",
    "    '2' : 'Details of Post Offices not performded EOD in DPMS',\n",
    "    '3' : 'Details of Bos for which  BO slips are not generated in CSI',\n",
    "    '4' : 'Details of Post offices where both NSP 1 & 2 are down',\n",
    "    '23' : 'Details of Post offices where both NSP 1 & 2 are down but ticket is not raised',\n",
    "    '5' : 'Details of BOs which have not logged in Darpan during Office Hours(9AM - 2PM)'\n",
    "}\n",
    "\n",
    "divisions = {\n",
    "    'Cachar Division' : ['Cachar'],\n",
    "    'Darrang Division' : ['Darrang'],\n",
    "    'Dibrugarh Division' : ['Dibrugarh'],\n",
    "    'Goalpara Division' : ['Goalpara'],\n",
    "    'Guwahati Division' : ['Guwahati', 'Guahati'],\n",
    "    'Nagaon Division' : ['Nagaon'],\n",
    "    'Nalbari Division' : ['Nalbari'],\n",
    "    'Sibsagar Division' : ['Sibsagar', 'Sivasagar', 'Sivsagar', 'Shivsagar'],\n",
    "    'Tinsukia Division' : ['Tinsukia']\n",
    "}\n",
    "\n",
    "# Check if logs csv exists\n",
    "if not os.path.exists(\"logs.csv\"):\n",
    "    df = pd.DataFrame(columns=[\"date\", \"report_id\", \"report_name\", \"status\", \"log_date\"])\n",
    "    df.to_csv(\"logs.csv\", index=False)\n",
    "\n",
    "# url encoding\n",
    "rpdate = date.replace(\"/\", \"%2f\")\n",
    "\n",
    "base_url = \"https://mis.cept.gov.in/viewDetails.aspx\"\n",
    "\n",
    "url = base_url + \"?cname=\" + cname + \"&rpdate=\" + rpdate\n",
    "webpage = requests.get(url)\n",
    "\n",
    "# Declare form id\n",
    "form_id = \"form1\"\n",
    "\n",
    "# Find form\n",
    "form = BeautifulSoup(webpage.text, \"html.parser\").find(\"form\", {\"id\": form_id})\n",
    "\n",
    "# get the inputs\n",
    "inputs = form.find_all(\"input\")\n",
    "\n",
    "# create a dictionary of the inputs\n",
    "input_dict = {}\n",
    "accepted_inputs = [\"__VIEWSTATE\", \"__VIEWSTATEGENERATOR\", \"__EVENTVALIDATION\"]\n",
    "for i in inputs:\n",
    "    if i.has_attr(\"value\") and i[\"name\"] in accepted_inputs:\n",
    "        input_dict[i[\"name\"]] = i[\"value\"]\n",
    "\n",
    "# Create headers\n",
    "headers = {\n",
    "    \"authority\": \"mis.cept.gov.in\",\n",
    "    \"method\": \"POST\",\n",
    "    \"path\": \"/viewDetails.aspx?cname=\" + cname + \"&rpdate=\" + rpdate,\n",
    "    \"scheme\": \"https\"\n",
    "}\n",
    "\n",
    "# Create a folder by the date inside data folder\n",
    "if not os.path.exists(\"data/\" + date.replace(\"/\", \"-\")):\n",
    "    os.makedirs(\"data/\" + date.replace(\"/\", \"-\"))\n",
    "\n",
    "# check if consolidated excel file exists for the date\n",
    "if not os.path.exists(\"data/\" + date.replace(\"/\", \"-\") + \"/consolidated.xlsx\"):\n",
    "    # Create a consolidated excel file\n",
    "    consolidated_df = pd.DataFrame(columns=[\"Division Name\", \"POS High Sync Defaulter dated \" + mobilink_date, \"POS Daily Sync Defaulter dated \" + mobilink_date ])\n",
    "\n",
    "    # Add report names as column names\n",
    "    for key, value in reports.items():\n",
    "        # Remove \"Details of \" from the report name and suffix it with date\n",
    "        val = value.replace(\"Details of \", \"\") + \" dated: \" + date\n",
    "        consolidated_df[val] = \"\"\n",
    "\n",
    "    #  Add division names as rows using concat\n",
    "    for key, value in divisions.items():\n",
    "        consolidated_df = pd.concat([consolidated_df, pd.DataFrame([[key]], columns=['Division Name'])])\n",
    "\n",
    "    # Save the file\n",
    "    consolidated_df.to_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/consolidated.xlsx\", index=False)\n",
    "    \n",
    "\n",
    "# Loop through the reports\n",
    "for key, value in reports.items():\n",
    "\n",
    "    # Check if the report has already been sent\n",
    "    logs_df = pd.read_csv(\"logs.csv\")\n",
    "    if logs_df[(logs_df[\"date\"] == date) & (logs_df[\"report_id\"] == int(key))].shape[0] > 0:\n",
    "        # enter in the logs\n",
    "        # logs_df = logs_df.append({\n",
    "        #     \"date\": date,\n",
    "        #     \"report_id\": key,\n",
    "        #     \"report_name\": value,\n",
    "        #     \"status\": \"skipped\",\n",
    "        #     \"log_date\": datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        # }, ignore_index=True)\n",
    "        # Append method has been deprecated, use concat instead\n",
    "        logs_df = pd.concat([logs_df, pd.DataFrame([[date, key, value, \"skipped\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")]], columns=[\"date\", \"report_id\", \"report_name\", \"status\", \"log_date\"])])\n",
    "        logs_df.to_csv(\"logs.csv\", index=False)\n",
    "        # skip the report\n",
    "        continue\n",
    "\n",
    "    # Create a dictionary of the fields\n",
    "    fields = {\n",
    "        \"ctl00$ContentPlaceHolder1$DdoTransaction\": key,\n",
    "        \"ctl00$ContentPlaceHolder1$DdoComplaint\" : cname,\n",
    "        \"ctl00$ContentPlaceHolder1$TxtReportDate\" : date,\n",
    "        \"ctl00$ContentPlaceHolder1$btnDelete\" : \"View Data\"\n",
    "    }\n",
    "\n",
    "    # Combine the inputs and fields\n",
    "    form_data = {**input_dict, **fields}\n",
    "\n",
    "    # Send the request\n",
    "    response = requests.post(url, data=form_data, headers=headers)\n",
    "\n",
    "    # Get the table\n",
    "    table = BeautifulSoup(response.text, \"html.parser\").find(\"table\", {\"id\": \"ContentPlaceHolder1_gvAll\"})\n",
    "\n",
    "    # Check if the table is empty\n",
    "    if table is None:\n",
    "        # Create text file with the report name\n",
    "        with open(\"data/\" + date.replace(\"/\", \"-\") + \"/\" + value + \"-\" + date.replace(\"/\", \"-\") + \".txt\", \"w\") as f:\n",
    "            f.write(\"No Data\")\n",
    "        continue\n",
    "\n",
    "    # Convert the table to a dataframe\n",
    "    df = pd.read_html(str(table))[0]\n",
    "\n",
    "    # Convert the dataframe to a excel file\n",
    "    df.to_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/\" + value + \" \" + date.replace(\"/\", \"-\") + \".xlsx\", index=False)\n",
    "\n",
    "    # Field name to group by\n",
    "    field_name = \"DIVISIONNAME\"\n",
    "\n",
    "    # Group by the field name\n",
    "    grouped = df.groupby(field_name).count()['CIRCLENAME']\n",
    "    grouped.loc['Total'] = grouped.sum()\n",
    "\n",
    "    # Rename the column CIRCLENAME to OFFICES\n",
    "    grouped.rename(\"OFFICES\", inplace=True)\n",
    "\n",
    "    # Save the grouped data to an excel file\n",
    "    grouped.to_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/\" + value + \" \" + date.replace(\"/\", \"-\") + \" grouped.xlsx\")\n",
    "    \n",
    "    consolidated_df = pd.read_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/consolidated.xlsx\")\n",
    "\n",
    "    # Loop throught he grouped rows\n",
    "    for index, row in grouped.items():\n",
    "        # first word of the index is the division name\n",
    "        div_name = index.split(\" \")[0]\n",
    "\n",
    "        name = ''\n",
    "\n",
    "        # Search the div_name in the lists of divisions \n",
    "        for k, v in divisions.items():\n",
    "            for div in v:\n",
    "                # case insensitive search\n",
    "                if div.lower() == div_name.lower():\n",
    "                    name = k\n",
    "                    break\n",
    "\n",
    "        # Remove \"Details of \" from the report name and suffix it with date\n",
    "        val = value.replace(\"Details of \", \"\") + \" dated: \" + date\n",
    "        \n",
    "        # Update the consolidated_df where the division name is like the name and the column name is like the value\n",
    "        consolidated_df.loc[consolidated_df[\"Division Name\"] == name, val] = row\n",
    "\n",
    "        # Add total row\n",
    "        # consolidated_df.loc[consolidated_df[\"Division Name\"].str.contains(\"Total\", case=False), value] = grouped.sum()\n",
    "        # Save the file\n",
    "        consolidated_df.to_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/consolidated.xlsx\", index=False)\n",
    "\n",
    "    # Add the log to the csv\n",
    "    logs_df = pd.read_csv(\"logs.csv\")\n",
    "    logs_df = pd.concat([logs_df, pd.DataFrame([[date, key, value, \"success\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")]], columns=logs_df.columns)])\n",
    "    logs_df.to_csv(\"logs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for csv file with the date in mobilink folder\n",
    "if not os.path.exists(\"mobilink/\" + mobilink_date.replace(\"/\", \"-\") + \".csv\"):\n",
    "    print(\"Mobilink file does not exist\")\n",
    "else:\n",
    "    print(\"Mobilink file exists\")\n",
    "    mobilink_df = pd.read_csv(\"mobilink/\" + mobilink_date.replace(\"/\", \"-\") + \".csv\")\n",
    "    # Trim the spaces from the columns\n",
    "    mobilink_df[\"CIRCLE\"] = mobilink_df[\"CIRCLE\"].str.strip()\n",
    "    mobilink_df[\"HIGH_SYNC_STATUS\"] = mobilink_df[\"HIGH_SYNC_STATUS\"].str.strip()\n",
    "    mobilink_df[\"DAILY_SYNC_STATUS\"] = mobilink_df[\"DAILY_SYNC_STATUS\"].str.strip()\n",
    "    mobilink_df[\"DIVISION\"] = mobilink_df[\"DIVISION\"].str.strip()\n",
    "\n",
    "\n",
    "    # Remove data where the CIRCLE is not \"Assam Circle\"\n",
    "    mobilink_df = mobilink_df[mobilink_df[\"CIRCLE\"] == \"Assam Circle\"]\n",
    "    # Remove data where the HIGH_SYNC_STATUS & DAILY_SYNC_STATUS is REGULAR\n",
    "    mobilink_df = mobilink_df[(mobilink_df[\"HIGH_SYNC_STATUS\"] == \"DEFAULTER\") | (mobilink_df[\"DAILY_SYNC_STATUS\"] == \"DEFAULTER\")]\n",
    "\n",
    "    # Replace the csv file with the new data\n",
    "    mobilink_df.to_csv(\"mobilink/\" + mobilink_date.replace(\"/\", \"-\") + \".csv\", index=False)\n",
    "\n",
    "    high_df = mobilink_df[mobilink_df[\"HIGH_SYNC_STATUS\"] == \"DEFAULTER\"]\n",
    "    daily_df = mobilink_df[mobilink_df[\"DAILY_SYNC_STATUS\"] == \"DEFAULTER\"]\n",
    "\n",
    "    grouped_high = high_df.groupby(\"DIVISION\").count()[\"CIRCLE\"]\n",
    "    grouped_daily = daily_df.groupby(\"DIVISION\").count()[\"CIRCLE\"]\n",
    "\n",
    "    consolidated_df = pd.read_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/consolidated.xlsx\")\n",
    "\n",
    "    for index, row in grouped_high.items():\n",
    "        div_name = index.split(\" \")[0]\n",
    "        name = ''\n",
    "        # Search the div_name in the lists of divisions \n",
    "        for k, v in divisions.items():\n",
    "            for div in v:\n",
    "                # case insensitive search\n",
    "                if div.lower() == div_name.lower():\n",
    "                    name = k\n",
    "                    break\n",
    "                \n",
    "        consolidated_df.loc[consolidated_df[\"Division Name\"] == name, \"POS High Sync Defaulter dated \" + mobilink_date] = row\n",
    "\n",
    "\n",
    "    for index, row in grouped_daily.items():\n",
    "        div_name = index.split(\" \")[0]\n",
    "        name = ''\n",
    "        # Search the div_name in the lists of divisions \n",
    "        for k, v in divisions.items():\n",
    "            for div in v:\n",
    "                # case insensitive search\n",
    "                if div.lower() == div_name.lower():\n",
    "                    name = k\n",
    "                    break\n",
    "                \n",
    "        consolidated_df.loc[consolidated_df[\"Division Name\"] == name, \"POS Daily Sync Defaulter dated \" + mobilink_date] = row\n",
    "\n",
    "    # Replace NaN with 0\n",
    "    consolidated_df = consolidated_df.fillna(0)\n",
    "\n",
    "    # Index starts from 1 instead of 0\n",
    "    consolidated_df.index += 1\n",
    "\n",
    "\n",
    "    # Add a total row\n",
    "    consolidated_df.loc[\"Total\"] = consolidated_df.sum()\n",
    "\n",
    "    # Change Division Name to of the last row to empty\n",
    "    consolidated_df[\"Division Name\"].iloc[-1] = \"\"\n",
    "\n",
    "    consolidated_df.to_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/consolidated.xlsx\")\n",
    "    \n",
    "# consolidated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"Reports/\" + date.replace(\"/\", \"-\") + \"_consolidated_data\" + \".xlsx\",) as writer:\n",
    "# Loop through the reports\n",
    "    for key, value in reports.items():\n",
    "        # load the excel file\n",
    "        xyz_df = pd.read_excel(\"data/\" + date.replace(\"/\", \"-\") + \"/\" + value + \" \" + date.replace(\"/\", \"-\") + \".xlsx\")\n",
    "        # save the file\n",
    "        xyz_df.to_excel(writer, sheet_name=value[11:], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the consolidated_df to a html table\n",
    "html = consolidated_df.to_html(justify=\"justify-all\", classes=\"table table-bordered table-striped table-hover\").replace('<tr>', '<tr align=\"center\">').replace('.0</td>', '</td>').replace('<td>0</td>', '<td style=\"background:lightgreen\">0</td>')\n",
    "\n",
    "# print(html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89f5398215e4bd68194ecfb3274f78f2b2188f90ccad3a120f4c53eb3abccfec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
